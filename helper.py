"""
helper.py - –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è IEEE CIS Fraud Detection

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç:
- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã
- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏
- –ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, StratifiedShuffleSplit, cross_validate
from sklearn.metrics import (
    roc_auc_score, classification_report, confusion_matrix, 
    accuracy_score, precision_score, recall_score, f1_score,
    roc_curve, precision_recall_curve, average_precision_score
)
from sklearn.feature_selection import mutual_info_classif
from matplotlib.patches import Patch
import warnings
warnings.filterwarnings('ignore')
from typing import List


# ============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –ò –ë–ê–ó–û–í–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•
# ============================================================================

def load_data(transaction_path, identity_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö.
    
    Parameters:
    -----------
    transaction_path : str
        –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º–∏
    identity_path : str
        –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        
    Returns:
    --------
    pd.DataFrame
        –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    """
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    transactions = pd.read_csv(transaction_path)
    identity = pd.read_csv(identity_path)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ TransactionID
    data = transactions.merge(identity, on='TransactionID', how='left')
    print(f"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {data.shape}")
    return data


import re

def get_feature_groups(data):
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —Ç–∏–ø–∞–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    –†–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ _ –∏ -
    """
    def get_prefix(col):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å —Å—Ç–æ–ª–±—Ü–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è"""
        return re.match(r'^[a-zA-Z]+', col).group() if re.match(r'^[a-zA-Z]+', col) else ''
    
    v_features = [col for col in data.columns if get_prefix(col).upper() == 'V']
    c_features = [col for col in data.columns if get_prefix(col).upper() == 'C']
    d_features = [col for col in data.columns if get_prefix(col).upper() == 'D']
    m_features = [col for col in data.columns if get_prefix(col).upper() == 'M']
    card_features = [col for col in data.columns if get_prefix(col).lower() == 'card']
    addr_features = [col for col in data.columns if get_prefix(col).lower() == 'addr']
    id_features = [col for col in data.columns if get_prefix(col).lower() == 'id']
    
    return {
        'v_features': v_features,
        'c_features': c_features,
        'd_features': d_features,
        'm_features': m_features,
        'id_features': id_features,
        'card_features': card_features,
        'addr_features': addr_features
    }

def create_feature_groups(df):
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä—É–ø–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º + base + target. –ü–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ."""
    
    # –ü—Ä–µ—Ñ–∏–∫—Å—ã
    prefixes = ['id_', 'card', 'addr', 'C', 'D', 'M', 'V']
    
    # –ì—Ä—É–ø–ø—ã –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º
    feature_groups = {}
    for prefix in prefixes:
        cols = sorted([col for col in df.columns if col.startswith(prefix)])
        if cols:
            feature_groups[f'{prefix}_features'] = cols
    
    # ‚úÖ –ò–°–ö–õ–Æ–ß–ò–¢–¨ DeviceInfo, DeviceType –∏–∑ D_features
    if 'D_features' in feature_groups:
        feature_groups['D_features'] = [col for col in feature_groups['D_features'] 
                                      if col not in ['DeviceInfo', 'DeviceType']]
    
    # Base + target
    base_cols = ['TransactionID', 'TransactionDT', 'TransactionAmt']
    target_col = 'isFraud'
    
    feature_groups['base_features'] = [col for col in base_cols if col in df.columns]
    feature_groups['target_features'] = [target_col] if target_col in df.columns else []
    
    # –û—Å—Ç–∞–ª—å–Ω–æ–µ + DeviceInfo, DeviceType –≤ 'other_features'
    grouped = set().union(*feature_groups.values())
    device_cols = [col for col in ['DeviceInfo', 'DeviceType'] if col in df.columns]
    other = sorted((set(df.columns) - grouped) | set(device_cols))
    feature_groups['other_features'] = other
    
    return feature_groups

def missing_summary(df: pd.DataFrame) -> pd.DataFrame:
    """–û–±—â–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    mis = df.isna().sum()
    mis = mis[mis > 0].sort_values(ascending=False)
    res = pd.DataFrame({
        'Missing_Count': mis,
        'Missing_Percent': (mis / len(df) * 100).round(2)
    })
    return res

def group_missing_summary(df: pd.DataFrame, groups: dict[str, List[str]]) -> pd.DataFrame:
    """–°–≤–æ–¥–∫–∞ –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º –ø–æ –≥—Ä—É–ø–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    rows = []
    n = len(df)
    
    for name, cols in sorted(groups.items()):
        cols_in_df = [c for c in cols if c in df.columns]
        if not cols_in_df:
            continue
            
        sub = df[cols_in_df]
        missing_count = sub.isna().sum().sum()
        total_cells = n * len(cols_in_df)
        affected_features = (sub.isna().sum() > 0).sum()
        
        rows.append({
            'Group': name,
            'Feature_Count': len(cols_in_df),
            'Missing_Count': int(missing_count),
            'Missing_Percent': round(missing_count / total_cells * 100, 2),
            'Affected_Features': affected_features,
        })
    
    return pd.DataFrame(rows).sort_values('Missing_Percent', ascending=False).reset_index(drop=True)



def split_feature_stats(df: pd.DataFrame, features: list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:
    sub = df[features]

    # ===== –ß–ò–°–õ–û–í–´–ï =====
    num = sub.select_dtypes(include=['number'])
    if not num.empty:
        desc_num = num.describe().T
        missing_pct_num = num.isna().mean() * 100
        unique_num = num.nunique(dropna=True)

        num_stats = desc_num.copy()
        num_stats["Feature"] = num_stats.index
        num_stats["Type"] = num.dtypes
        num_stats["Missing_%"] = missing_pct_num.round(2)
        num_stats["Unique"] = unique_num

        num_stats = (
            num_stats[[
                "Feature", "Type", "Missing_%", "Unique",
                "mean", "std", "min", "25%", "50%", "75%", "max"
            ]]
            .rename(columns={
                "mean": "Mean",
                "std": "Std",
                "min": "Min",
                "max": "Max"
            })
            .reset_index(drop=True)
        )

        # –æ–∫—Ä—É–≥–ª—è–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ —Å–æ—Ç—ã—Ö
        num_cols_to_round = ["Mean", "Std", "Min", "25%", "50%", "75%", "Max"]
        num_stats[num_cols_to_round] = num_stats[num_cols_to_round].round(2)

    else:
        num_stats = pd.DataFrame(columns=[
            "Feature", "Type", "Missing_%", "Unique",
            "Mean", "Std", "Min", "25%", "50%", "75%", "Max"
        ])

    # ===== OBJECT / CATEGORY =====
    obj = sub.select_dtypes(include=['object', 'category'])
    if not obj.empty:
        desc_obj = obj.describe().T
        missing_pct_obj = obj.isna().mean() * 100

        obj_stats = desc_obj.copy()
        obj_stats["Feature"] = obj_stats.index
        obj_stats["Type"] = obj.dtypes
        obj_stats["Missing_%"] = missing_pct_obj.round(2)

        obj_stats = (
            obj_stats[[
                "Feature", "Type", "Missing_%", "count", "unique", "top", "freq"
            ]]
            .rename(columns={
                "count": "Count",
                "unique": "Unique",
                "top": "Top",
                "freq": "Freq"
            })
            .reset_index(drop=True)
        )
    else:
        obj_stats = pd.DataFrame(columns=[
            "Feature", "Type", "Missing_%", "Count", "Unique", "Top", "Freq"
        ])

    return num_stats, obj_stats



def run_fraud_feature_report(train: pd.DataFrame):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω –∏–∑ –Ω–æ—É—Ç–±—É–∫–∞:
    0) CONFIG (–≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏)
    1) Stratified sample
    2) Corr (Pearson + Spearman)
    3) MI (top –ø–æ corr + missing flags + median impute)
    4) D-features: mean/median diff + missing_rate
    5) Nonlinearity: fraud_rate range –ø–æ –∫–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–º –±–∏–Ω–∞–º

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.
    """

    # -----------------------------
    # CONFIG
    # -----------------------------
    SEED = 42
    SAMPLE_SIZE = 100_000
    TOP_CORR_N = 200          # —Å–∫–æ–ª—å–∫–æ –≤–∑—è—Ç—å –ø–æ corr –¥–ª—è MI
    MI_TOP_N = 30
    CORR_THR = 0.01
    N_BINS = 10               # –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏ (–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ)
    TARGET = "isFraud"
    ID_COL = "TransactionID"

    np.random.seed(SEED)

    # -----------------------------
    # HELPERS
    # -----------------------------
    def make_stratified_idx(y, n, seed=42):
        """–ß—Ç–æ–±—ã –≤ sample –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–æ–ø–∞–ª–∏ fraud'—ã (–∏–Ω–∞—á–µ MI –º–æ–∂–µ—Ç –ø–ª–∞–≤–∞—Ç—å)."""
        n = min(n, len(y))
        sss = StratifiedShuffleSplit(n_splits=1, train_size=n, random_state=seed)
        idx, _ = next(sss.split(np.zeros(len(y)), y))
        return idx

    def pretty_head(df, n=20, title=None):
        if title:
            print("\n" + title)
            print("-" * len(title))
        if display is not None:
            display(df.head(n))
        else:
            print(df.head(n))

    def binned_fraud_rate(df, feat, y_col=TARGET, bins=10):
        s = df[[feat, y_col]].copy()
        s = s.dropna(subset=[feat])
        # –µ—Å–ª–∏ –º–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ‚Äî qcut –º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å; –æ–±—Ä–∞–±–æ—Ç–∞–µ–º
        try:
            s["bin"] = pd.qcut(s[feat], q=bins, duplicates="drop")
        except ValueError:
            return None
        g = s.groupby("bin")[y_col].agg(["count", "mean"]).rename(columns={"mean": "fraud_rate"})
        g["fraud_rate_pct"] = g["fraud_rate"] * 100
        return g.reset_index()

    # -----------------------------
    # PREP COLS
    # -----------------------------
    numeric_cols = (
        train.select_dtypes(include=[np.number])
             .columns
             .drop([ID_COL, TARGET], errors="ignore")
             .tolist()
    )

    y = train[TARGET].astype(int)
    idx = make_stratified_idx(y, SAMPLE_SIZE, seed=SEED)

    print(f"Rows: {len(train):,} | Fraud rate: {y.mean()*100:.2f}% | Sample: {len(idx):,}")

    # -----------------------------
    # 1) CORRELATIONS (Pearson + Spearman)
    # -----------------------------
    corr_pearson = train[numeric_cols].corrwith(y).abs().sort_values(ascending=False)
    corr_spearman = train[numeric_cols].corrwith(y, method="spearman").abs().sort_values(ascending=False)

    corr_df = pd.DataFrame({
        "pearson_abs": corr_pearson,
        "spearman_abs": corr_spearman
    }).sort_values(["pearson_abs", "spearman_abs"], ascending=False)

    corr_filtered = corr_df[(corr_df["pearson_abs"] > CORR_THR) | (corr_df["spearman_abs"] > CORR_THR)]
    pretty_head(corr_filtered, 30, title=f"Top correlations (abs > {CORR_THR})")

    # -----------------------------
    # 2) MUTUAL INFORMATION (–Ω–∞ top –ø–æ corr + missing flags)
    # -----------------------------
    top_cols = corr_df.head(TOP_CORR_N).index.tolist()

    X = train[top_cols].iloc[idx].copy()
    y_s = y.iloc[idx].copy()

    # missing flags ‚Äî –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –ø–æ–ª–µ–∑–Ω–æ –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã—Ö fraud-–¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
    X_miss = X.isna().astype(np.uint8).add_prefix("miss__")
    X = pd.concat([X, X_miss], axis=1)

    # –ø—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: NaN -> median (–∏–ª–∏ 0, –Ω–æ median –æ–±—ã—á–Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ)
    X = X.fillna(X.median(numeric_only=True))

    mi = mutual_info_classif(
        X, y_s,
        n_neighbors=5,
        random_state=SEED,
        n_jobs=-1
    )

    mi_df = (pd.DataFrame({"feature": X.columns, "mi": mi})
               .sort_values("mi", ascending=False)
               .reset_index(drop=True))

    pretty_head(mi_df, MI_TOP_N, title=f"Mutual Info top-{MI_TOP_N} (sample stratified, top-{TOP_CORR_N} corr + missing flags)")

    # -----------------------------
    # 3) D-features quick scan (mean/median + missing + effect size)
    # -----------------------------
    d_feats = [f"D{i}" for i in range(1, 16) if f"D{i}" in train.columns]

    if d_feats:
        tmp = train[[TARGET] + d_feats].copy()
        miss_rate = tmp[d_feats].isna().mean().rename("missing_rate")

        grp_mean = tmp.groupby(TARGET)[d_feats].mean().T.rename(columns={0: "mean_0", 1: "mean_1"})
        grp_med  = tmp.groupby(TARGET)[d_feats].median().T.rename(columns={0: "med_0", 1: "med_1"})

        d_summary = (grp_mean.join(grp_med)
                            .join(miss_rate)
                            .assign(
                                abs_mean_diff=lambda d: (d["mean_1"] - d["mean_0"]).abs(),
                                abs_med_diff =lambda d: (d["med_1"]  - d["med_0"]).abs()
                            )
                            .sort_values(["abs_med_diff", "abs_mean_diff"], ascending=False)
                            .reset_index()
                            .rename(columns={"index": "feature"}))

        pretty_head(d_summary, 15, title="D-features: mean/median diff + missing_rate")
    else:
        d_summary = None
        print("No D-features found")

    # -----------------------------
    # 4) –ù–ï–õ–ò–ù–ï–ô–ù–û–°–¢–¨: fraud_rate –ø–æ –±–∏–Ω–∞–º (quantiles)
    # -----------------------------
    # –±–µ—Ä—ë–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª—É—á—à–∏—Ö –ø–æ MI (–±–µ–∑ miss__ —Ñ–∏—á)
    top_real_feats = [f for f in mi_df["feature"].head(10).tolist() if not f.startswith("miss__")]

    nonlinear_rows = []
    for f in top_real_feats:
        g = binned_fraud_rate(train[[f, TARGET]], f, TARGET, bins=N_BINS)
        if g is None:
            continue
        # –ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ "–Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏": —Ä–∞–∑–±—Ä–æ—Å fraud rate –ø–æ –±–∏–Ω–∞–º
        fr_range = g["fraud_rate"].max() - g["fraud_rate"].min()
        nonlinear_rows.append((f, fr_range, g["count"].min(), g["count"].sum()))

    nonlinear_df = (pd.DataFrame(nonlinear_rows, columns=["feature", "fraud_rate_range", "min_bin_n", "total_n"])
                      .sort_values("fraud_rate_range", ascending=False))

    pretty_head(nonlinear_df, 10, title="Nonlinearity check: fraud_rate range across quantile bins (top MI feats)")

    return {
        "config": {
            "SEED": SEED, "SAMPLE_SIZE": SAMPLE_SIZE, "TOP_CORR_N": TOP_CORR_N, "MI_TOP_N": MI_TOP_N,
            "CORR_THR": CORR_THR, "N_BINS": N_BINS, "TARGET": TARGET, "ID_COL": ID_COL
        },
        "sample_idx": idx,
        "numeric_cols": numeric_cols,
        "corr_all": corr_df,
        "corr_filtered": corr_filtered,
        "mi": mi_df,
        "d_summary": d_summary,
        "nonlinear": nonlinear_df,
        "top_real_feats_for_nonlin": top_real_feats,
    }

def reduce_correlated_features(
    df: pd.DataFrame,
    features: list[str],
    corr_thresh: float = 0.75,
    group_by: str = "nan_count",   # "nan_count" (–±—ã—Å—Ç—Ä–æ) –∏–ª–∏ "nan_pattern" (—Ç–æ—á–Ω–µ–µ)
    min_group_size: int = 2,
):
    
    """
    –£–º–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —É—á—ë—Ç–æ–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤.
    
    –ê–ª–≥–æ—Ä–∏—Ç–º:
    1) –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ NaN (–æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ —á–∏—Å–ª–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏–ª–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω)
    2) –í–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –Ω–∞—Ö–æ–¥–∏—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ >corr_thresh
    3) –í –∫–∞–∂–¥–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–µ –æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º nunique (–Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π)
    """
    
    # 1) –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ NaN-—Å—Ç—Ä—É–∫—Ç—É—Ä–µ
    groups = {}
    for col in features:
        s = df[col]
        if group_by == "nan_count":
            key = int(s.isna().sum())
        elif group_by == "nan_pattern":
            # —Ö—ç—à –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (—á—Ç–æ–±—ã –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—å –æ–≥—Ä–æ–º–Ω—ã–µ –±—É–ª–µ–≤—ã –≤–µ–∫—Ç–æ—Ä–∞)
            key = int(pd.util.hash_pandas_object(s.isna(), index=False).sum())
        else:
            raise ValueError("group_by must be 'nan_count' or 'nan_pattern'")
        groups.setdefault(key, []).append(col)

    keep = []
    drop = []
    components_debug = []  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫–∏–µ –≥—Ä—É–ø–ø—ã —Å—Ö–ª–æ–ø–Ω—É–ª–∏—Å—å

    # 2) –í–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —Ä–µ–∂–µ–º –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    for _, cols in groups.items():
        if len(cols) < min_group_size:
            keep.extend(cols)
            continue

        corr = df[cols].corr().abs().fillna(0.0)
        cols_set = set(cols)
        visited = set()

        for c in cols:
            if c not in cols_set or c in visited:
                continue

            # BFS/DFS: –Ω–∞—Ö–æ–¥–∏–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –ø–æ —Ä–µ–±—Ä–∞–º |corr|>thr
            stack = [c]
            comp = []
            visited.add(c)

            while stack:
                u = stack.pop()
                comp.append(u)

                neigh = corr.index[(corr.loc[u] > corr_thresh)].tolist()
                for v in neigh:
                    if v in cols_set and v not in visited:
                        visited.add(v)
                        stack.append(v)

            if len(comp) == 1:
                keep.append(comp[0])
                continue

            # 3) –í –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–µ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ñ–∏—á—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º nunique
            nunique = df[comp].nunique(dropna=True)
            best = nunique.idxmax()

            keep.append(best)
            to_drop = [x for x in comp if x != best]
            drop.extend(to_drop)

            components_debug.append({"component": comp, "kept": best, "dropped": to_drop})

    keep = sorted(set(keep), key=keep.index) if len(keep) else []
    drop = sorted(set(drop))
    return keep, drop, components_debug


def create_temporal_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å —Å—É–º–º–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏.
    
    –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫—Ä–∏—Ç–∏—á–Ω—ã –¥–ª—è fraud detection, —Ç–∞–∫ –∫–∞–∫:
    - –ú–æ—à–µ–Ω–Ω–∏–∫–∏ –∞–∫—Ç–∏–≤–Ω–µ–µ –Ω–æ—á—å—é (–º–µ–Ω—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—è, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ç–∞–∫–∏)
    - –ë–æ–ª—å—à–∏–µ —Å—É–º–º—ã –Ω–æ—á—å—é = –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
    - –î–µ–Ω—å –Ω–µ–¥–µ–ª–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ fraud rate (–≤—ã—Ö–æ–¥–Ω—ã–µ vs –±—É–¥–Ω–∏)
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    ----------
    df : pd.DataFrame
        –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ TransactionDT –∏ TransactionAmt
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    -----------
    df : pd.DataFrame
        –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ 12 –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    
    –°–æ–∑–¥–∞–≤–∞–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:
    ---------------------
    –ë–∞–∑–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ (5):
    - hour_of_day : int (0-23) ‚Äî —á–∞—Å —Å–æ–≤–µ—Ä—à–µ–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
    - day_of_week : int (0-6) ‚Äî –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (0=–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫, 6=–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)
    - day_number : int ‚Äî –Ω–æ–º–µ—Ä –¥–Ω—è –æ—Ç –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–∏–æ–¥–∞
    - is_night : int (0/1) ‚Äî —Ñ–ª–∞–≥ –Ω–æ—á–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (22:00-06:00)
    - is_early_morning : int (0/1) ‚Äî —Ñ–ª–∞–≥ —Ä–∞–Ω–Ω–µ–≥–æ —É—Ç—Ä–∞ (00:00-04:00)
    - is_weekend : int (0/1) ‚Äî —Ñ–ª–∞–≥ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –¥–Ω—è (—Å–±-–≤—Å)
    - time_period : category ‚Äî –ø–µ—Ä–∏–æ–¥ –¥–Ω—è (night/morning/afternoon/evening)
    
    –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å TransactionAmt (5):
    - night_high_amount : int (0/1) ‚Äî –Ω–æ—á—å √ó –±–æ–ª—å—à–∞—è —Å—É–º–º–∞ (>$500)
    - TransactionAmt_log : float ‚Äî log1p(TransactionAmt)
    - night_amount_log : float ‚Äî –Ω–æ—á—å √ó log(—Å—É–º–º–∞)
    - hour_amount_log : float ‚Äî —á–∞—Å √ó log(—Å—É–º–º–∞)
    - suspicious_night_tx : int (0/1) ‚Äî –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—á–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è (>$200)
    - time_amt_category : category ‚Äî –ø–µ—Ä–∏–æ–¥ –¥–Ω—è √ó –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å—É–º–º—ã
    
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    ---------------------
    train = create_temporal_features(train)
    test = create_temporal_features(test)
    
    print(f"–°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {train.shape[1]}")
    """
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    if 'TransactionDT' not in df.columns or 'TransactionAmt' not in df.columns:
        print("‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω—ã TransactionDT –∏–ª–∏ TransactionAmt ‚Äî –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã")
        return df
    
    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏
    SECONDS_PER_DAY = 86400   # 60 * 60 * 24
    SECONDS_PER_HOUR = 3600   # 60 * 60
    
    # =========================================================
    # –ë–ê–ó–û–í–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    # =========================================================
    
    # –ß–∞—Å —Å—É—Ç–æ–∫ (0-23)
    # TransactionDT % 86400 = —Å–µ–∫—É–Ω–¥—ã –≤–Ω—É—Ç—Ä–∏ –¥–Ω—è ‚Üí –¥–µ–ª–∏–º –Ω–∞ 3600 = —á–∞—Å—ã
    df['hour_of_day'] = ((df['TransactionDT'] % SECONDS_PER_DAY) / SECONDS_PER_HOUR).astype(int)
    
    # –î–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (0-6, –≥–¥–µ 0 = –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫)
    # TransactionDT // 86400 = –Ω–æ–º–µ—Ä –¥–Ω—è –æ—Ç –Ω–∞—á–∞–ª–∞ ‚Üí –±–µ—Ä—ë–º –æ—Å—Ç–∞—Ç–æ–∫ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 7
    df['day_of_week'] = (df['TransactionDT'] // SECONDS_PER_DAY) % 7
    
    # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –Ω–æ–º–µ—Ä –¥–Ω—è –æ—Ç –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–∏–æ–¥–∞
    # –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è temporal drift (–∏–∑–º–µ–Ω–µ–Ω–∏–µ fraud rate —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)
    df['day_number'] = (df['TransactionDT'] // SECONDS_PER_DAY).astype(int)
    
    # =========================================================
    # –§–õ–ê–ì–ò –í–†–ï–ú–ï–ù–ò –°–£–¢–û–ö
    # =========================================================
    
    # –ù–æ—á—å (22:00-06:00) ‚Äî –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ fraud
    # –ú–æ—à–µ–Ω–Ω–∏–∫–∏ –∞–∫—Ç–∏–≤–Ω–µ–µ –Ω–æ—á—å—é: –º–µ–Ω—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—è, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—Ç–∞–∫–∏
    df['is_night'] = ((df['hour_of_day'] >= 22) | (df['hour_of_day'] < 6)).astype(int)
    
    # –†–∞–Ω–Ω–µ–µ —É—Ç—Ä–æ (00:00-04:00) ‚Äî —Å–∞–º—ã–π –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥
    # –õ–µ–≥–∏—Ç–∏–º–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Ä–µ–¥–∫–æ —Å–æ–≤–µ—Ä—à–∞—é—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ –≤ 2-4 —á–∞—Å–∞ –Ω–æ—á–∏
    df['is_early_morning'] = (df['hour_of_day'] < 4).astype(int)
    
    # –í—ã—Ö–æ–¥–Ω—ã–µ (—Å—É–±–±–æ—Ç–∞-–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)
    # Fraud rate –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –≤ –≤—ã—Ö–æ–¥–Ω—ã–µ vs –±—É–¥–Ω–∏
    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
    
    # –ü–µ—Ä–∏–æ–¥ –¥–Ω—è (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤)
    # night (00-06), morning (06-12), afternoon (12-18), evening (18-24)
    df['time_period'] = pd.cut(
        df['hour_of_day'],
        bins=[-1, 6, 12, 18, 24],  # –≥—Ä–∞–Ω–∏—Ü—ã –ø–µ—Ä–∏–æ–¥–æ–≤
        labels=['night', 'morning', 'afternoon', 'evening']
    ).astype('category')
    
    # =========================================================
    # –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø –° TransactionAmt
    # =========================================================
    
    # –ù–æ—á—å √ó –±–æ–ª—å—à–∞—è —Å—É–º–º–∞ (>$500)
    # –ü–∞—Ç—Ç–µ—Ä–Ω: –ª–µ–≥–∏—Ç–∏–º–Ω—ã–µ –∫—Ä—É–ø–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏ —Ä–µ–¥–∫–æ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –Ω–æ—á—å—é
    # Fraud —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–∫—Ä–∞–¥–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å—É–º–º –∏–º–µ–Ω–Ω–æ –Ω–æ—á—å—é
    df['night_high_amount'] = (
        df['is_night'] * (df['TransactionAmt'] > 500)
    ).astype(int)
    
    # –°–æ–∑–¥–∞—ë–º log(TransactionAmt), –µ—Å–ª–∏ –µ–≥–æ –µ—â—ë –Ω–µ—Ç
    # log1p = log(1 + x) ‚Äî –∏–∑–±–µ–≥–∞–µ–º log(0) –∏ —Å–∂–∏–º–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
    if 'TransactionAmt_log' not in df.columns:
        df['TransactionAmt_log'] = np.log1p(df['TransactionAmt'])
    
    # –ù–æ—á—å √ó log(—Å—É–º–º–∞)
    # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ: —á–µ–º –±–æ–ª—å—à–µ —Å—É–º–º–∞ –Ω–æ—á—å—é, —Ç–µ–º –≤—ã—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    df['night_amount_log'] = df['is_night'] * df['TransactionAmt_log']
    
    # –ß–∞—Å √ó log(—Å—É–º–º–∞)
    # –ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω "–≤ –∫–∞–∫–æ–µ –≤—Ä–µ–º—è –∫–∞–∫–∏–µ —Å—É–º–º—ã —Ç–∏–ø–∏—á–Ω—ã"
    # –ù–∞–ø—Ä–∏–º–µ—Ä: —É—Ç—Ä–æ–º ‚Äî –º–∞–ª–µ–Ω—å–∫–∏–µ (–∫–æ—Ñ–µ), –¥–Ω—ë–º ‚Äî —Å—Ä–µ–¥–Ω–∏–µ (–æ–±–µ–¥—ã), –≤–µ—á–µ—Ä–æ–º ‚Äî –±–æ–ª—å—à–∏–µ (—Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã)
    df['hour_amount_log'] = df['hour_of_day'] * df['TransactionAmt_log']
    
    # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—á–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è (–Ω–æ—á—å + —Å—É–º–º–∞ >$200)
    # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥, —á–µ–º night_high_amount
    # –ù–æ—á—å—é –¥–∞–∂–µ —Å—Ä–µ–¥–Ω–∏–µ —Å—É–º–º—ã –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã
    df['suspicious_night_tx'] = (
        (df['is_night'] == 1) & (df['TransactionAmt'] > 200)
    ).astype(int)
    
    # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥ –¥–Ω—è √ó –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å—É–º–º—ã (–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫)
    # –ü—Ä–∏–º–µ—Ä—ã: "night_vhigh", "morning_low", "afternoon_mid"
    # –î–ª—è –¥–µ—Ä–µ–≤—å–µ–≤: –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—á–µ—Å—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    df['time_amt_category'] = (
        df['time_period'].astype(str) + '_' + 
        pd.cut(
            df['TransactionAmt'], 
            bins=[0, 50, 200, 1000, np.inf],  # –≥—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å—É–º–º—ã
            labels=['low', 'mid', 'high', 'vhigh']
        ).astype(str)
    ).astype('category')
    
    # =========================================================
    # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
    # =========================================================
    
    print("‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ + –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å TransactionAmt: 12 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"   üìä –ë–∞–∑–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ: 7 (hour, day_of_week, day_number, is_night, is_early_morning, is_weekend, time_period)")
    print(f"   üìä –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: 5 (night_high_amount, night_amount_log, hour_amount_log, suspicious_night_tx, time_amt_category)")
    print(f"\n   üåô –ù–æ—á–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {df['is_night'].sum():,} ({df['is_night'].mean()*100:.1f}%)")
    print(f"   üåô –ù–æ—á–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π >$500: {df['night_high_amount'].sum():,} ({df['night_high_amount'].mean()*100:.2f}%)")
    print(f"   üåô –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –Ω–æ—á–Ω—ã—Ö: {df['suspicious_night_tx'].sum():,} ({df['suspicious_night_tx'].mean()*100:.2f}%)")
    print(f"   üìÖ –í—ã—Ö–æ–¥–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {df['is_weekend'].sum():,} ({df['is_weekend'].mean()*100:.1f}%)")
    
    return df


def create_transaction_amount_features(df: pd.DataFrame, quantiles_bins: list = None) -> tuple:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ TransactionAmt –ë–ï–ó –£–¢–ï–ß–ö–ò –¥–∞–Ω–Ω—ã—Ö.
    
    –í–ê–ñ–ù–û: –î–ª—è train –≤—ã—á–∏—Å–ª—è–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏, –¥–ª—è test –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –≥—Ä–∞–Ω–∏—Ü—ã!
    –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage (—É—Ç–µ—á–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ test –≤ train).
    
    –°–æ–∑–¥–∞–≤–∞–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:
    1. TransactionAmt_log ‚Äî log1p(—Å—É–º–º–∞) –¥–ª—è —Å–∂–∞—Ç–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
    2. TransactionAmt_sqrt ‚Äî sqrt(—Å—É–º–º–∞) –¥–ª—è —Å–∂–∞—Ç–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ (–º—è–≥—á–µ log)
    3. TransactionAmt_bin ‚Äî –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≥—Ä–∞–Ω–∏—Ü–∞–º (micro/small/medium/high)
    4. TransactionAmt_decile ‚Äî –¥–µ—Ü–∏–ª–∏ (0-9) –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    ----------
    df : pd.DataFrame
        –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –ø—Ä–∏–∑–Ω–∞–∫–æ–º TransactionAmt
    quantiles_bins : list, optional
        –ì—Ä–∞–Ω–∏—Ü—ã –¥–µ—Ü–∏–ª–µ–π, –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞ train (–¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∫ test)
        –ï—Å–ª–∏ None, –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ (–¥–ª—è train)
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    -----------
    df : pd.DataFrame
        –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ 4 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    decile_bins : list
        –ì—Ä–∞–Ω–∏—Ü—ã –¥–µ—Ü–∏–ª–µ–π (–¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∫ test)
    
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    ---------------------
    # –î–ª—è train: –≤—ã—á–∏—Å–ª—è–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏
    train, train_decile_bins = create_transaction_amount_features(train)
    
    # –î–ª—è test: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –≥—Ä–∞–Ω–∏—Ü—ã
    test, _ = create_transaction_amount_features(test, quantiles_bins=train_decile_bins)
    
    –ó–ê–ß–ï–ú –ù–£–ñ–ù–´ –≠–¢–ò –ü–†–ò–ó–ù–ê–ö–ò:
    -------------------------
    1. log/sqrt: TransactionAmt –∏–º–µ–µ—Ç –≤—ã–±—Ä–æ—Å—ã ($0.25 –¥–æ $31,937)
       - –õ–æ–≥–∞—Ä–∏—Ñ–º —Å–∂–∏–º–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Å—É–º–º—ã, –¥–µ–ª–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–æ–ª–µ–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º
       - –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ω–æ –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
    
    2. –ë–∏–Ω—ã (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏): –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –ø–æ—Ä–æ–≥–∏
       - "micro" ($0-10): –ø–æ–¥–∞—Ä–æ—á–Ω—ã–µ –∫–∞—Ä—Ç—ã, –º–µ–ª–∫–∏–µ –ø–æ–∫—É–ø–∫–∏
       - "extreme" ($1000+): –∫—Ä—É–ø–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏ (—Ç—Ä–µ–±—É—é—Ç –æ—Å–æ–±–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è)
    
    3. –î–µ—Ü–∏–ª–∏: —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ 10 –≥—Ä—É–ø–ø
       - –ö–∞–∂–¥–∞—è –≥—Ä—É–ø–ø–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç ~10% —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
       - –ú–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏—Ç fraud rate –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
    """
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è TransactionAmt
    if 'TransactionAmt' not in df.columns:
        print("‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ TransactionAmt –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return df, None
    
    # =========================================================
    # 1. –õ–û–ì–ê–†–ò–§–ú (log1p –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è log(0))
    # =========================================================
    # log1p(x) = log(1 + x) ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –≤–µ—Ä—Å–∏—è log, —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –Ω—É–ª—è–º–∏
    # –°–∂–∏–º–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Å—É–º–º—ã: $10,000 ‚Üí 9.21, $1,000 ‚Üí 6.91, $100 ‚Üí 4.62
    # –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
    df['TransactionAmt_log'] = np.log1p(df['TransactionAmt'])
    
    # =========================================================
    # 2. –ö–í–ê–î–†–ê–¢–ù–´–ô –ö–û–†–ï–ù–¨
    # =========================================================
    # sqrt(x) ‚Äî –º—è–≥—á–µ —Å–∂–∏–º–∞–µ—Ç –≤—ã–±—Ä–æ—Å—ã, —á–µ–º log
    # –ü—Ä–∏–º–µ—Ä: $10,000 ‚Üí 100, $1,000 ‚Üí 31.62, $100 ‚Üí 10
    # –ò–Ω–æ–≥–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ log –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    df['TransactionAmt_sqrt'] = np.sqrt(df['TransactionAmt'])
    
    # =========================================================
    # 3. –ö–ê–¢–ï–ì–û–†–ò–ò –ü–û –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ú –ì–†–ê–ù–ò–¶–ê–ú (bins)
    # =========================================================
    # –†–∞–∑–±–∏–≤–∞–µ–º —Å—É–º–º—ã –Ω–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–µ:
    # - micro ($0-10): –º–µ–ª–∫–∏–µ –ø–æ–∫—É–ø–∫–∏ (–∫–æ—Ñ–µ, —Å–Ω–µ–∫–∏, –ø–æ–¥–∞—Ä–æ—á–Ω—ã–µ –∫–∞—Ä—Ç—ã)
    # - small ($10-50): –æ–±—ã—á–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏ (—Ñ–∞—Å—Ç—Ñ—É–¥, —Ç–∞–∫—Å–∏)
    # - medium ($50-100): —Å—Ä–µ–¥–Ω–∏–µ –ø–æ–∫—É–ø–∫–∏ (–æ–¥–µ–∂–¥–∞, –∫–Ω–∏–≥–∏)
    # - medium_high ($100-200): –∫—Ä—É–ø–Ω—ã–µ –ø–æ–∫—É–ø–∫–∏ (—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è)
    # - high ($200-500): –¥–æ—Ä–æ–≥–∏–µ –ø–æ–∫—É–ø–∫–∏ (—Å–º–∞—Ä—Ç—Ñ–æ–Ω—ã, –ø–ª–∞–Ω—à–µ—Ç—ã)
    # - very_high ($500-1000): –æ—á–µ–Ω—å –¥–æ—Ä–æ–≥–∏–µ –ø–æ–∫—É–ø–∫–∏ (–Ω–æ—É—Ç–±—É–∫–∏, —é–≤–µ–ª–∏—Ä–∫–∞)
    # - extreme ($1000+): —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –¥–æ—Ä–æ–≥–∏–µ (–º–µ–±–µ–ª—å, –±—ã—Ç–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞)
    #
    # –í–ê–ñ–ù–û: –≥—Ä–∞–Ω–∏—Ü—ã —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ (–Ω–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –¥–∞–Ω–Ω—ã—Ö) ‚Üí –Ω–µ—Ç —É—Ç–µ—á–∫–∏
    df['TransactionAmt_bin'] = pd.cut(
        df['TransactionAmt'], 
        bins=[0, 10, 50, 100, 200, 500, 1000, 10000],
        labels=['micro', 'small', 'medium', 'medium_high', 'high', 'very_high', 'extreme']
    ).astype('category')
    
    # =========================================================
    # 4. –î–ï–¶–ò–õ–ò (10 —Ä–∞–≤–Ω—ã—Ö –≥—Ä—É–ø–ø –ø–æ —Ä–∞–∑–º–µ—Ä—É)
    # =========================================================
    # –†–∞–∑–±–∏–≤–∞–µ–º TransactionAmt –Ω–∞ 10 –≥—Ä—É–ø–ø, –∫–∞–∂–¥–∞—è —Å–æ–¥–µ—Ä–∂–∏—Ç ~10% —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞–π—Ç–∏ fraud rate –¥–ª—è –∫–∞–∂–¥–æ–π –¥–µ—Ü–∏–ª–∏
    #
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –¥–ª—è train –≤—ã—á–∏—Å–ª—è–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏, –¥–ª—è test –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ!
    # –ò–Ω–∞—á–µ –±—É–¥–µ—Ç data leakage (—É—Ç–µ—á–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ test –≤ train)
    
    if quantiles_bins is None:
        # –î–ª—è TRAIN: –≤—ã—á–∏—Å–ª—è–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏ (–≥—Ä–∞–Ω–∏—Ü—ã –¥–µ—Ü–∏–ª–µ–π)
        quantiles = df['TransactionAmt'].quantile([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]).tolist()
        decile_bins = [df['TransactionAmt'].min() - 0.01] + quantiles + [df['TransactionAmt'].max() + 1]
    else:
        # –î–ª—è TEST: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã, –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞ train
        decile_bins = quantiles_bins
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –¥–µ—Ü–∏–ª–∏
    # labels=False ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–º–µ—Ä–∞ –≥—Ä—É–ø–ø (0, 1, 2, ..., 9)
    # duplicates='drop' ‚Üí –µ—Å–ª–∏ –≥—Ä–∞–Ω–∏—Ü—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç (—Ä–µ–¥–∫–æ), —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    # include_lowest=True ‚Üí –≤–∫–ª—é—á–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –ø–µ—Ä–≤—É—é –≥—Ä—É–ø–ø—É
    df['TransactionAmt_decile'] = pd.cut(
        df['TransactionAmt'], 
        bins=decile_bins, 
        labels=False,
        duplicates='drop',
        include_lowest=True
    )
    
    # =========================================================
    # –°–¢–ê–¢–ò–°–¢–ò–ö–ê
    # =========================================================
    print("‚úÖ TransactionAmt: –¥–æ–±–∞–≤–ª–µ–Ω–æ 4 –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞ (–ë–ï–ó –£–¢–ï–ß–ö–ò)")
    print(f"   üìä TransactionAmt_log: min={df['TransactionAmt_log'].min():.2f}, max={df['TransactionAmt_log'].max():.2f}")
    print(f"   üìä TransactionAmt_sqrt: min={df['TransactionAmt_sqrt'].min():.2f}, max={df['TransactionAmt_sqrt'].max():.2f}")
    print(f"   üìä TransactionAmt_bin: {df['TransactionAmt_bin'].nunique()} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
    print(f"      –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {df['TransactionAmt_bin'].value_counts().to_dict()}")
    print(f"   üìä TransactionAmt_decile: 10 –≥—Ä—É–ø–ø (0-9)")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –≥—Ä–∞–Ω–∏—Ü –¥–µ—Ü–∏–ª–µ–π –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
    if quantiles_bins is None:
        decile_labels = [f'${x:.2f}' for x in decile_bins[:5]]
        print(f"      –î–µ—Ü–∏–ª—å–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã (–ø–µ—Ä–≤—ã–µ 5): {decile_labels}... (–≤—Å–µ–≥–æ {len(decile_bins)} –≥—Ä–∞–Ω–∏—Ü)")
    else:
        print(f"      –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –≥—Ä–∞–Ω–∏—Ü—ã –∏–∑ train (–≤—Å–µ–≥–æ {len(decile_bins)} –≥—Ä–∞–Ω–∏—Ü)")
    
    print(f"\nüéØ –ò—Ç–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {df.shape[1]}")
    
    return df, decile_bins



def get_categorical_features(df):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è CatBoost
    –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ IEEE-CIS Fraud Detection
    """
    # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ IEEE-CIS
    official_categorical = [
        'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',
        'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',
        'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',
        'DeviceType', 'DeviceInfo',
        'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19',
        'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27',
        'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35',
        'id_36', 'id_37', 'id_38'
    ]
    
    categorical_features = [col for col in official_categorical if col in df.columns]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å dtype='category'
    for col in df.columns:
        if df[col].dtype.name == 'category' and col not in categorical_features:
            categorical_features.append(col)
            print(f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è category –∫–æ–ª–æ–Ω–∫–∞: {col}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å dtype='object'
    for col in df.columns:
        if df[col].dtype == 'object' and col not in categorical_features:
            categorical_features.append(col)
            print(f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–∞ object (string) –∫–æ–ª–æ–Ω–∫–∞: {col}")
    
    return categorical_features


def prepare_categorical_features(X, categorical_features):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Å—Ç—Ä–æ–∫–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç NaN
    
    Parameters:
    -----------
    X : DataFrame
        –î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    categorical_features : list
        –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
    Returns:
    --------
    X_processed : DataFrame
        –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
    categorical_features : list
        –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    print(f"\nüîß –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    
    X_processed = X.copy()
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    for col in categorical_features:
        if col in X_processed.columns:
            # –ï—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ category dtype, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ str
            if X_processed[col].dtype.name == 'category':
                X_processed[col] = X_processed[col].astype(str)
            
            # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 'missing' –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ str
            X_processed[col] = X_processed[col].fillna('missing').astype(str)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏—â–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
    for col in X_processed.columns:
        if col not in categorical_features:
            if X_processed[col].dtype == 'object':
                print(f"   ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï! –ö–æ–ª–æ–Ω–∫–∞ '{col}' –∏–º–µ–µ—Ç dtype='object', –Ω–æ –Ω–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö")
                print(f"      –ü—Ä–∏–º–µ—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π: {X_processed[col].dropna().head(3).tolist()}")
                
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ float
                try:
                    X_processed[col] = pd.to_numeric(X_processed[col], errors='coerce')
                    print(f"      ‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø")
                except:
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è - –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
                    categorical_features.append(col)
                    X_processed[col] = X_processed[col].fillna('missing').astype(str)
                    print(f"      ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —á–∏—Å–ª–æ–≤–æ–π - –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ")
    
    print(f"\n‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ {len(categorical_features)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    return X_processed, categorical_features



# ============================================================================
# –í–ê–õ–ò–î–ê–¶–ò–û–ù–ù–´–ï –°–¢–†–ê–¢–ï–ì–ò–ò
# ============================================================================

def get_time_series_split(data, n_splits=5):
    """
    TimeSeriesSplit –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞.
    –í–∞–∂–Ω–æ: –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ train, –ø–æ–∑–¥–Ω–∏–µ –≤ test.
    
    Parameters:
    -----------
    data : pd.DataFrame
        –î–∞—Ç–∞—Å–µ—Ç (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
    n_splits : int
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤
        
    Returns:
    --------
    list of tuples
        [(train_idx, test_idx), ...]
    """
    tss = TimeSeriesSplit(n_splits=n_splits)
    splits = list(tss.split(data))
    print(f"TimeSeriesSplit: {n_splits} —Ñ–æ–ª–¥–æ–≤")
    for i, (train_idx, test_idx) in enumerate(splits):
        print(f"  –§–æ–ª–¥ {i+1}: train={len(train_idx)}, test={len(test_idx)}")
    return splits



# ============================================================================
# –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô
# ============================================================================

def evaluate_model(y_true, y_pred_proba, y_pred=None, threshold=0.5):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
    
    Parameters:
    -----------
    y_true : array-like
        –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    y_pred_proba : array-like
        –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∞ 1
    y_pred : array-like, optional
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (–≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –µ—Å–ª–∏ –Ω–µ –¥–∞–Ω—ã)
    threshold : float
        –ü–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        
    Returns:
    --------
    dict
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    if y_pred is None:
        y_pred = (y_pred_proba >= threshold).astype(int)
    
    auc = roc_auc_score(y_true, y_pred_proba)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    metrics = {
        'AUC': auc,
        'Precision': precision,
        'Recall': recall,
        'F1': f1
    }
    
    return metrics


def cross_val_evaluate(model, X, y, cv, stratified=False):
    """
    –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫.
    
    Parameters:
    -----------
    model : sklearn model
        –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    X : pd.DataFrame
        –ü—Ä–∏–∑–Ω–∞–∫–∏
    y : pd.Series
        –¢–∞—Ä–≥–µ—Ç
    cv : int or cross-validator
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
    stratified : bool
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å StratifiedKFold?
        
    Returns:
    --------
    dict
        –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —Ñ–æ–ª–¥–∞–º
    """
    scoring = {
        'auc': 'roc_auc',
        'precision': 'precision',
        'recall': 'recall',
        'f1': 'f1'
    }
    
    cv_results = cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=True)
    
    result = {}
    for metric in scoring.keys():
        test_scores = cv_results[f'test_{metric}']
        result[metric] = {
            'mean': test_scores.mean(),
            'std': test_scores.std(),
            'scores': test_scores
        }
    
    return result


# ============================================================================
# –†–ê–ë–û–¢–ê –° –í–†–ï–ú–ï–ù–ï–ú
# ============================================================================

def extract_time_features(data, datetime_col='TransactionDT'):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ Timedelta –∫–æ–ª–æ–Ω–∫–∏.
    
    Parameters:
    -----------
    data : pd.DataFrame
        –î–∞—Ç–∞—Å–µ—Ç
    datetime_col : str
        –ò–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –≤—Ä–µ–º–µ–Ω–µ–º
        
    Returns:
    --------
    pd.DataFrame
        –î–∞—Ç–∞—Å–µ—Ç —Å –Ω–æ–≤—ã–º–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    data_copy = data.copy()
    
    # TransactionDT –≤ —Å–µ–∫—É–Ω–¥–∞—Ö, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∞—Å—ã –∏ –¥–Ω–∏
    if datetime_col in data_copy.columns:
        data_copy['hour'] = (data_copy[datetime_col] // 3600) % 24
        data_copy['day_of_week'] = (data_copy[datetime_col] // 86400) % 7
        data_copy['day_of_month'] = (data_copy[datetime_col] // 86400) % 30
        
    return data_copy


def get_sorted_by_time(data, time_col='TransactionDT'):
    """
    –°–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–ª—è TimeSeriesSplit).
    
    Parameters:
    -----------
    data : pd.DataFrame
    time_col : str
        –ö–æ–ª–æ–Ω–∫–∞ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
        
    Returns:
    --------
    pd.DataFrame
        –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    """
    return data.sort_values(by=time_col).reset_index(drop=True)


# ============================================================================
# –ê–ù–ê–õ–ò–ó –î–ò–°–ë–ê–õ–ê–ù–°–ê
# ============================================================================

def analyze_class_balance(y, name=""):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤.
    
    Parameters:
    -----------
    y : pd.Series or array-like
        –¢–∞—Ä–≥–µ—Ç
    name : str
        –ù–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –≤—ã–≤–æ–¥–∞
    """
    counts = pd.Series(y).value_counts()
    proportions = pd.Series(y).value_counts(normalize=True) * 100
    
    print(f"\n{name} –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤:")
    for cls in sorted(counts.index):
        print(f"  –ö–ª–∞—Å—Å {cls}: {counts[cls]} ({proportions[cls]:.2f}%)")


# ============================================================================
# –£–¢–ò–õ–ò–¢–´ –î–õ–Ø FEATURE ENGINEERING
# ============================================================================

def create_uid(data, cols=['card1', 'addr1', 'D1']):
    """
    –°–æ–∑–¥–∞—ë—Ç Unique Identifier –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.
    –≠—Ç–æ –≥–ª–∞–≤–Ω–∞—è "–º–∞–≥–∏—è" –≤—ã–∏–≥—Ä—ã–≤–∞—é—â–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è!
    
    Parameters:
    -----------
    data : pd.DataFrame
    cols : list
        –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è UID
        
    Returns:
    --------
    pd.Series
        UID –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
    """
    data_copy = data.copy()
    for col in cols:
        if col not in data_copy.columns:
            print(f"–í–Ω–∏–º–∞–Ω–∏–µ: –∫–æ–ª–æ–Ω–∫–∞ {col} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
    uid = data_copy[cols[0]].astype(str)
    for col in cols[1:]:
        uid = uid + '_' + data_copy[col].astype(str)
    
    return uid


def frequency_encode(data, col):
    """
    –§—Ä–µ–∫–≤–µ–Ω—Ç–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.
    –°—á–∏—Ç–∞–µ—Ç, –∫–∞–∫ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –∫–∞–∂–¥–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.
    
    Parameters:
    -----------
    data : pd.DataFrame
    col : str
        –ö–æ–ª–æ–Ω–∫–∞ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        
    Returns:
    --------
    pd.Series
        –§—Ä–µ–∫–≤–µ–Ω—Ç–Ω—ã–µ –∫–æ–¥—ã
    """
    freq_map = data[col].value_counts().to_dict()
    return data[col].map(freq_map).fillna(0).astype(int)


def handle_missing_values(data, d_features, fill_value=-1):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ D-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö.
    D –ø—Ä–∏–∑–Ω–∞–∫–∏ - —ç—Ç–æ timedeltas, –ø—Ä–æ–ø—É—Å–∫ –∑–Ω–∞—á–∏—Ç –ø–µ—Ä–≤–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è.
    
    Parameters:
    -----------
    data : pd.DataFrame
    d_features : list
        –°–ø–∏—Å–æ–∫ D –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    fill_value : int
        –ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
        
    Returns:
    --------
    pd.DataFrame
    """
    data_copy = data.copy()
    for col in d_features:
        if col in data_copy.columns:
            data_copy[col].fillna(fill_value, inplace=True)
    
    return data_copy


def analyze_object_columns(data):
    """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ —Ç–∏–ø–∞ object"""
    object_cols = data.select_dtypes(include='object').columns
    
    print(f"–í—Å–µ–≥–æ —Å—Ç–æ–ª–±—Ü–æ–≤ object: {len(object_cols)}\n")
    
    for col in object_cols:
        print(f"{'='*60}")
        print(f"–°—Ç–æ–ª–±–µ—Ü: {col}")
        print(f"{'='*60}")
        print(f"–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {data[col].dtype}")
        print(f"–í—Å–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–π: {len(data[col])}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {data[col].nunique()}")
        print(f"–ü—Ä–æ–ø—É—Å–∫–æ–≤: {data[col].isnull().sum()} ({data[col].isnull().sum() / len(data) * 100:.2f}%)")
        print(f"–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
        print(data[col].value_counts().head(5))
        print()
        
def analyze_numeric_columns(data):
    """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ (int –∏ float)"""
    numeric_cols = data.select_dtypes(include=['int64', 'int32', 'float64', 'float32']).columns
    
    print(f"–í—Å–µ–≥–æ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤: {len(numeric_cols)}\n")
    
    for col in numeric_cols:
        print(f"{'='*60}")
        print(f"–°—Ç–æ–ª–±–µ—Ü: {col} ({data[col].dtype})")
        print(f"{'='*60}")
        print(f"–í—Å–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–π: {len(data[col])}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {data[col].nunique()}")
        print(f"–ü—Ä–æ–ø—É—Å–∫–æ–≤: {data[col].isnull().sum()} ({data[col].isnull().sum() / len(data) * 100:.2f}%)")
        print(f"Min: {data[col].min():.4f}")
        print(f"Max: {data[col].max():.4f}")
        print(f"Mean: {data[col].mean():.4f}")
        print(f"Median: {data[col].median():.4f}")
        print(f"Std: {data[col].std():.4f}")
        print()


# ============================================================================
# PRINT –£–¢–ò–õ–ò–¢–´
# ============================================================================

def print_section(title, char='='):
    """–ü–µ—á–∞—Ç–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞."""
    print(f"\n{char * 70}")
    print(f"  {title}")
    print(f"{char * 70}\n")


def print_results(results_dict, metric_name="–ú–µ—Ç—Ä–∏–∫–∞"):
    """–ü–µ—á–∞—Ç–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
    print(f"\n{metric_name}:")
    for model_name, metrics in results_dict.items():
        print(f"\n  {model_name}:")
        for metric, value in metrics.items():
            if isinstance(value, dict):
                print(f"    {metric}: {value['mean']:.4f} (+/- {value['std']:.4f})")
            else:
                print(f"    {metric}: {value:.4f}")
                
def analyze_missing_by_groups(data):
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø–æ –≥—Ä—É–ø–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    feature_groups = get_feature_groups(data)
    
    results = []
    for group_name, features in feature_groups.items():
        if features:  # –ï—Å–ª–∏ –≥—Ä—É–ø–ø–∞ –Ω–µ –ø—É—Å—Ç–∞
            group_data = data[features]
            missing_count = group_data.isnull().sum().sum()
            missing_percent = (missing_count / (len(data) * len(features))) * 100
            
            results.append({
                'Group': group_name,
                'Feature_Count': len(features),
                'Missing_Count': missing_count,
                'Missing_Percent': round(missing_percent, 2),
                'Affected_Features': (group_data.isnull().sum() > 0).sum()
            })
    
    return pd.DataFrame(results).sort_values('Missing_Percent', ascending=False)



def get_dataset_stats(data):
    """
    –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    
    Parameters:
    -----------
    data : pd.DataFrame
        –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    
    Returns:
    --------
    dict : –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        - to_drop_high_missing: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å >80% –ø—Ä–æ–ø—É—Å–∫–æ–≤
        - to_drop_constants: —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        - to_check: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (50-80% –ø—Ä–æ–ø—É—Å–∫–æ–≤)
        - binary_features: —Å–ø–∏—Å–æ–∫ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        - categorical_features: —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        - numeric_features: —Å–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        - possible_ids: —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö ID
        - stats: –æ–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    """
    
    print("="*100)
    print("–ö–†–ê–¢–ö–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–†–ò–ó–ù–ê–ö–ê–ú")
    print("="*100)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç–∏–ø—ã
    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
    
    print(f"\nüìä –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:   {data.shape[1]}")
    print(f"üìä –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫:       {data.shape[0]:,}")
    print(f"üìà –ß–∏—Å–ª–æ–≤—ã—Ö:          {len(numeric_cols)}")
    print(f"üìù –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö:    {len(categorical_cols)}")
    
    # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    print("\n" + "="*100)
    print("üìã –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò")
    print("="*100 + "\n")
    
    total_missing = data.isnull().sum().sum()
    total_cells = data.shape[0] * data.shape[1]
    
    print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<40} {'–ó–Ω–∞—á–µ–Ω–∏–µ':<20}")
    print("-"*60)
    print(f"{'–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏':<40} {data.isnull().any().sum():<20}")
    print(f"{'–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤':<40} {data.shape[1] - data.isnull().any().sum():<20}")
    print(f"{'–û–±—â–∏–π % –ø—Ä–æ–ø—É—Å–∫–æ–≤':<40} {total_missing / total_cells * 100:.2f}%")
    print(f"{'–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å <10 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö':<40} {sum(data.nunique() < 10):<20}")
    print(f"{'–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å >1000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö':<40} {sum(data.nunique() > 1000):<20}")
    
    # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    print("\n" + "="*100)
    print("‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò")
    print("="*100)
    
    # –ü—Ä–æ–ø—É—Å–∫–∏ > 80%
    high_missing = data.columns[data.isnull().sum() / len(data) > 0.8].tolist()
    print(f"\nüî¥ –ü—Ä–æ–ø—É—Å–∫–æ–≤ >80% ({len(high_missing)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
    if high_missing:
        for col in high_missing[:10]:
            pct = data[col].isnull().sum() / len(data) * 100
            print(f"   {col:<35} {pct:>6.2f}%")
        if len(high_missing) > 10:
            print(f"   ... –∏ –µ—â—ë {len(high_missing) - 10} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    else:
        print("   ‚úÖ –ù–µ—Ç")
    
    # –ü—Ä–æ–ø—É—Å–∫–∏ 50-80%
    medium_missing = data.columns[(data.isnull().sum() / len(data) > 0.5) & 
                                   (data.isnull().sum() / len(data) <= 0.8)].tolist()
    print(f"\nüü† –ü—Ä–æ–ø—É—Å–∫–æ–≤ 50-80% ({len(medium_missing)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
    if medium_missing:
        for col in medium_missing[:10]:
            pct = data[col].isnull().sum() / len(data) * 100
            print(f"   {col:<35} {pct:>6.2f}%")
        if len(medium_missing) > 10:
            print(f"   ... –∏ –µ—â—ë {len(medium_missing) - 10} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    else:
        print("   ‚úÖ –ù–µ—Ç")
    
    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    constants = data.columns[data.nunique() == 1].tolist()
    print(f"\n‚ö™ –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã (1 —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ) ({len(constants)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
    if constants:
        for col in constants:
            print(f"   {col}")
    else:
        print("   ‚úÖ –ù–µ—Ç")
    
    # –ë–∏–Ω–∞—Ä–Ω—ã–µ
    binary = data.columns[data.nunique() == 2].tolist()
    print(f"\nüü° –ë–∏–Ω–∞—Ä–Ω—ã–µ (2 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö) ({len(binary)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
    
    # –í–æ–∑–º–æ–∂–Ω—ã–µ ID
    possible_ids = []
    if len(categorical_cols) > 0:
        possible_ids = [col for col in categorical_cols if data[col].nunique() / len(data) > 0.9]
        print(f"\nüîµ –í–æ–∑–º–æ–∂–Ω—ã–µ ID (>90% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö) ({len(possible_ids)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
        if possible_ids:
            for col in possible_ids:
                unique_pct = data[col].nunique() / len(data) * 100
                print(f"   {col:<35} {unique_pct:>6.2f}% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
        else:
            print("   ‚úÖ –ù–µ—Ç")
    
    # –í—ã–≤–æ–¥—ã
    print("\n" + "="*100)
    print("üí° –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("="*100 + "\n")
    
    to_drop_80 = data.columns[data.isnull().sum() / len(data) > 0.8].tolist()
    to_drop_const = data.columns[data.nunique() == 1].tolist()
    
    print(f"‚ùå –£–î–ê–õ–ò–¢–¨ ({len(to_drop_80) + len(to_drop_const)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
    print(f"   - –ü—Ä–æ–ø—É—Å–∫–æ–≤ >80%: {len(to_drop_80)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print(f"   - –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã: {len(to_drop_const)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    to_check = data.columns[(data.isnull().sum() / len(data) > 0.5) & 
                            (data.isnull().sum() / len(data) <= 0.8)].tolist()
    print(f"\n‚ö†Ô∏è –ü–†–û–í–ï–†–ò–¢–¨ –í–ê–ñ–ù–û–°–¢–¨ ({len(to_check)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
    print(f"   - –ü—Ä–æ–ø—É—Å–∫–æ–≤ 50-80%: {len(to_check)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    print(f"\n‚úÖ –û–ë–†–ê–ë–û–¢–ê–¢–¨:")
    print(f"   - –ë–∏–Ω–∞—Ä–Ω—ã–µ (2 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö): {len(binary)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Üí –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å 0/1")
    print(f"   - –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ: {len(categorical_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Üí Label encoding –∏–ª–∏ One-hot")
    print(f"   - –ü—Ä–æ–ø—É—Å–∫–∏ <50%: —Å–æ–∑–¥–∞—Ç—å —Ñ–ª–∞–≥–∏ _is_missing, –∑–∞–ø–æ–ª–Ω–∏—Ç—å –º–µ–¥–∏–∞–Ω–æ–π/–º–æ–¥–æ–π")
    
    print("\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–Å–ù!")
    
    # –í–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = {
        'to_drop_high_missing': to_drop_80,
        'to_drop_constants': to_drop_const,
        'to_check': to_check,
        'binary_features': binary,
        'categorical_features': categorical_cols,
        'numeric_features': numeric_cols,
        'possible_ids': possible_ids,
        'stats': {
            'total_features': data.shape[1],
            'total_rows': data.shape[0],
            'numeric_features': len(numeric_cols),
            'categorical_features': len(categorical_cols),
            'features_with_missing': data.isnull().any().sum(),
            'total_missing_pct': total_missing / total_cells * 100,
            'binary_features': len(binary),
            'constant_features': len(constants)
        }
    }
    
    return results

def eda_cat_fraud_report(
    df,
    feats,
    target='isFraud',
    top_n=10,
    min_nobs=100,
    top_fraud_n=8,
    lift_thr=1.5,
    verbose=True
):
    """
    –ü–µ—á–∞—Ç–∞–µ—Ç EDA-–æ—Ç—á–µ—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º —Ñ–∏—á–∞–º (–∏ –ª—é–±—ã–º –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º/–≥—Ä—É–ø–ø–∏—Ä—É–µ–º—ã–º),
    —Å—á–∏—Ç–∞–µ—Ç fraud-rate –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ lift –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ baseline.
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç summary DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ —Ñ–∏—á–∞–º.
    """
    baseline = df[target].mean() * 100

    if verbose:
        print("üîç EDA –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –§–ò–ß + DIST (Fraud Rate –∞–Ω–∞–ª–∏–∑)")
        print("=" * 80)

    rows = []

    for feat in feats:
        if feat not in df.columns:
            if verbose:
                print(f"‚ùå {feat} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            continue

        s = df[feat]
        vc = s.value_counts(dropna=False)

        # crosstab –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        ct = pd.crosstab(s, df[target], normalize='index') * 100  # –ø—Ä–æ—Ü–µ–Ω—Ç—ã –ø–æ —Å—Ç—Ä–æ–∫–∞–º [web:101]
        ct['nobs'] = vc

        # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º nobs
        top_fraud = (
            ct[ct['nobs'] >= min_nobs]
            .sort_values(1, ascending=False)
            .head(top_fraud_n)
        )

        max_fraud = float(top_fraud[1].max()) if len(top_fraud) else np.nan
        lift = (max_fraud / baseline) if (baseline > 0 and not np.isnan(max_fraud)) else np.nan

        rec = "üî• –°–ò–õ–¨–ù–ê–Ø —Ñ–∏—á–∞" if (not np.isnan(lift) and lift > lift_thr) else "‚úÖ –•–æ—Ä–æ—à–∞—è/—Å–ª–∞–±–∞—è"

        miss_rate = s.isna().mean()
        nunique = s.nunique(dropna=True)

        rows.append({
            "feature": feat,
            "baseline_fraud_%": baseline,
            "max_fraud_%": max_fraud,
            "lift": lift,
            "missing_rate": miss_rate,
            "nunique": nunique,
            "recommendation": rec,
            "top_fraud_table": top_fraud.round(2)[[0, 1]] if len(top_fraud) else None,
            "top_counts": vc.head(top_n)  # –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º —Å–º–æ—Ç—Ä–µ—Ç—å
        })

        if verbose:
            print(f"\nüìä {feat.upper()}")
            print("-" * 50)
            print(f"–¢–æ–ø-{top_n}:\n{vc.head(top_n)}")

            if len(top_fraud):
                print(f"\nFraud % (—Ç–æ–ø, >={min_nobs} obs):\n{top_fraud.round(2)[[0,1]]}")
                print(f"üìà –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {rec} (max {max_fraud:.1f}% vs baseline {baseline:.1f}%, lift x{lift:.1f})")
            else:
                print(f"\nFraud %: –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å nobs >= {min_nobs}")
                print(f"üìà –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {rec} (baseline {baseline:.1f}%)")

            print(f"Missing: {miss_rate:.1%} | Unique: {nunique}")

    summary = pd.DataFrame(rows).sort_values("lift", ascending=False, na_position="last")

    if verbose:
        print("\nüéØ –ò–¢–û–ì: Target encode —Ç–æ–ø-3 –ø–æ lift + is_missing_* –±–∏–Ω–∞—Ä–∫–∏")

    return summary

def plot_corr_and_pairs(
    df,
    features,
    title='–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏',
    threshold=0.9,
    max_pairs=10,
    figsize=(10, 8),
    plot=True,
    print_pairs=True
):
    corr = df[features].corr()

    # 1) Heatmap
    if plot:
        plt.figure(figsize=figsize)
        sns.heatmap(
            corr, cmap='coolwarm', center=0, square=True,
            cbar_kws={'label': '–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è'}
        )
        plt.title(title, fontsize=12, fontweight='bold')
        plt.tight_layout()
        plt.show()

    # 2) –ü–∞—Ä—ã |r| > threshold (–≤–µ—Ä—Ö–Ω–∏–π —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫)
    cols = corr.columns
    pairs = []
    for i in range(len(cols)):
        for j in range(i + 1, len(cols)):
            r = corr.iloc[i, j]
            if abs(r) > threshold:
                pairs.append((cols[i], cols[j], float(r)))

    pairs_df = (pd.DataFrame(pairs, columns=['feat_1', 'feat_2', 'corr'])
                .sort_values('corr', key=lambda s: s.abs(), ascending=False))

    if print_pairs:
        print(f'\n–í—ã—Å–æ–∫–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã (|r| > {threshold}):')
        if len(pairs_df):
            for _, row in pairs_df.head(max_pairs).iterrows():
                print(f"{row['feat_1']} <-> {row['feat_2']}: {row['corr']:.4f}")
        else:
            print('–ù–µ—Ç –ø–∞—Ä –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞')

    # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä—ã (–Ω–µ –º–∞—Ç—Ä–∏—Ü—É)
    return pairs_df